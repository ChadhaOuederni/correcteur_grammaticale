#  Correction automatique de texte avec T5 et LoRA

## üìò Description du projet

Ce projet vise √† **entra√Æner et √©valuer un mod√®le T5** (Text-To-Text Transfer Transformer) pour effectuer une **correction de texte automatique**.  
L‚Äôobjectif est d‚Äôapprendre au mod√®le √† transformer une phrase "brute" ou "erron√©e" en une version "propre" ou "corrig√©e".  

Le mod√®le est affin√© gr√¢ce √† **LoRA (Low-Rank Adaptation)**, une technique l√©g√®re d‚Äôadaptation des poids qui permet de r√©duire les co√ªts m√©moire et d‚Äôentra√Ænement, tout en pr√©servant les performances.

Le script a √©t√© ex√©cut√© sur **Kaggle** avec GPU activ√©.

---

## ‚öôÔ∏è Mod√®le et outils utilis√©s

| √âl√©ment | Description |
|----------|--------------|
| **Mod√®le de base** | `t5-small` |
| **M√©thode d‚Äôadaptation** | LoRA (r=16, alpha=64, dropout=0.05) |
| **Tokenisation** | `T5Tokenizer` (max_length = 128) |
| **Frameworks** | Hugging Face Transformers, Datasets, PEFT |
| **Langage** | Python 3.10+ |
| **Environnement** | GPU (CUDA activ√© sur Kaggle) |

---

##  √âtapes principales du pipeline

### 1Ô∏è‚É£ Chargement et pr√©traitement des donn√©es
- Lecture du fichier `.tsv` par **chunks de 10 000 lignes** pour √©viter la surcharge m√©moire.  
- Colonnes : `text` (texte original) et `clean` (texte corrig√©).  
- Nettoyage des textes (`clean_text`) : suppression des espaces multiples et des caract√®res inutiles.  
- Filtrage des lignes selon un **ratio de longueur ‚â§ 2.0** pour √©liminer les paires d√©s√©quilibr√©es.

### 2Ô∏è‚É£ Conversion en Dataset Hugging Face
Chaque chunk est transform√© en un objet `Dataset`, puis tous sont concat√©n√©s.  
Une limite de **100 000 lignes** est fix√©e pour la d√©monstration.

R√©partition :
- **80 %** pour l‚Äôentra√Ænement (`train_dataset`)
- **20 %** pour le test (`test_dataset`)

---

### 3Ô∏è‚É£ Tokenisation
Les textes sont tokenis√©s avec `T5Tokenizer` :
- Longueur maximale : 128 tokens  
- Padding : `max_length`  
- Masquage des tokens de remplissage (`-100`) dans les labels pour le calcul de la perte.

---

### 4Ô∏è‚É£ Configuration LoRA
Une configuration **LoRA** est appliqu√©e sur les couches d‚Äôattention `q` et `v` du mod√®le :

```python
config = LoraConfig(
    r=16,
    lora_alpha=64,
    target_modules=["q", "v"],
    lora_dropout=0.05,
    bias="none",
    task_type="SEQ_2_SEQ_LM",
)
